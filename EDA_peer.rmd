---
title: "Peer Assessment I"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---


First, let us load the data and necessary packages:

```{r load, message = FALSE}
load("ames_train.Rdata")
library(MASS)
library(dplyr)
library(ggplot2)
library(scales)
library(GGally)
library(BAS)
```

#
Make a labeled histogram (with 30 bins) of the ages of the houses in the data set, and describe the distribution.


```{r Q1}
ames_train %>%
  mutate(age=2017-Year.Built) %>%
  select(age) %>% 
  ggplot(aes(x=age)) + 
  geom_histogram(bins=30) +
  ggtitle("Histogram by house years") +
  theme(axis.text.x =   element_text(angle=40,  size=12))  


```


* * *

The ages of the houses (taking 2017 as the actual value ) has a right skewed distribution.


* * *


#
The mantra in real estate is "Location, Location, Location!" Make a graphical display that relates a home price to its neighborhood in Ames, Iowa. Which summary statistics are most appropriate to use for determining the most expensive, least expensive, and most heterogeneous (having the most variation in housing price) neighborhoods? Report which neighborhoods these are based on the summary statistics of your choice. Report the value of your chosen summary statistics for these neighborhoods.


```{r Q2}
ames_train %>% 
  group_by(Neighborhood) %>% 
  ggplot(aes(x=Neighborhood,y=price))+
  geom_boxplot() +
  ggtitle("Price Variations by Neighborhood") +
  theme(axis.text.x =   element_text(angle=45,  size=10)) +
   scale_y_continuous( labels = comma)


```

Box plot of price by neighborhood shows us at a simple glance the variations and help us to compare them in a simple way.
Now we'll compute a summary grouped by neighborhood.

```{r}
nbh_dist<- ames_train %>% 
  group_by(Neighborhood) %>% 
  summarise(max=max(price), min=min(price),average=mean(price),median=median(price),std=sd(price))
data.frame(nbh_dist)  



```

Then we'll select top values for min, max, avg and standard devation for price variable.

```{r}
nbh_dist %>% select(Neighborhood,max)  %>% top_n(1)
nbh_dist %>% select(Neighborhood,min)  %>% top_n(-1) 
nbh_dist %>% select(Neighborhood,average)  %>% top_n(1)
nbh_dist %>% select(Neighborhood,std)  %>% top_n(1)
```


* * *

For this datset we can confirm that NridgHt has the more expensive house, Oldtown the cheapest. StoneBr has, in average, the more expensive houses and StoneBr has the more variable house price.



* * *

# 

Which variable has the largest number of missing values? Explain why it makes sense that there are so many missing values for this variable.

```{r Q3}
tmp<-data.frame(apply(ames_train, 2, function(x) length(which(is.na(x)))))
nas <- cbind(rownames(tmp),tmp);
rownames(nas)<-NULL;colnames(nas)<-c("Neighborhood","NAs")
nas %>% top_n(1)

```


* * *

NAs on Pool quality variable mean that the house does't have a swimming pool. So we can confirm that almost any house on Ames has a pool, at least for this observation dataset.


* * *

#

We want to predict the natural log of the home prices. Candidate explanatory variables are lot size in square feet (Lot.Area), slope of property (Land.Slope), original construction date (Year.Built), remodel date (Year.Remod.Add), and the number of bedrooms above grade (Bedroom.AbvGr). Pick a model selection or model averaging method covered in the Specialization, and describe how this method works. Then, use this method to find the best multiple regression model for predicting the natural log of the home prices.


We'll create a variable that contains the required columns for creating the regression model.


```{r Q4}
mod_ds<-ames_train %>% select(price,Lot.Area,Land.Slope,Year.Built,Year.Remod.Add,Bedroom.AbvGr)
ggpairs(mod_ds)

```



The plot tells us about the relationship between all the columns.

Now we'll create two different Bayesian models to analyse which colums to include in our model.
The first model set the prior probablity as BIC and the second uses Zellner-Siow Cauchy prior.


```{r}

 bma_mod_bic <- bas.lm(log(price) ~ . , data=mod_ds,prior = "BIC", 
                                       modelprior = uniform())

image(bma_mod_bic,rotate=F)

bma_mod_zel <- bas.lm(log(price) ~ . , 
                  data=mod_ds,prior = "ZS-null",  
                  modelprior = uniform())


image(bma_mod_zel,rotate=F)


```

We can see both model suggest to include all the columns and Zellner's approach has greater posterior odds.




```{r}


summary(bma_mod_zel)

```



* * *

Based on aour analysis we'll choose bma_mod_zel model that explains 56.2% of the variablity. The highest value of all the possible models.


* * *

#

Which home has the largest squared residual in the previous analysis (Question 4)? Looking at all the variables in the data set, can you explain why this home stands out from the rest (what factors contribute to the high squared residual and why are those factors relevant)?


```{r Q5}
lm_price <- lm(log(price)~Lot.Area+Land.Slope+Year.Built+Year.Remod.Add+Bedroom.AbvGr , data= mod_ds)
resDF<-data.frame(residuals=(abs(lm_price$residuals))^2)
mod_ds_res<-cbind(mod_ds ,resDF)
mod_ds_res %>% arrange(desc(residuals)) %>% head(1)

```

* * *
We've created a linear model including 6 columns, then we've got the squared residuals.
This home stands out from the rest mostly because it was built befor than 1930.


* * *

#

Use the same model selection method you chose in Question 4 to again find the best multiple regression model to predict the natural log of home prices, but this time **replacing Lot.Area with log(Lot.Area)**. Do you arrive at a model including the same set of predictors?


In order to answer this we'll create two new models (prior BIC and Zellner) but our explanatory variable Lot.Area will be treated as a natural log to evaluate how the model behaves and what should we keep or remove.

```{r Q6}
bma_mod_bic_log <- bas.lm(log(price) ~log( Lot.Area)+Land.Slope+Year.Built+Year.Remod.Add+Bedroom.AbvGr , 
                      data=mod_ds,prior = "BIC", 
                      modelprior = uniform())

image(bma_mod_bic_log,rotate=F)

bma_mod_zel_log <- bas.lm(log(price) ~ log(Lot.Area)+Land.Slope+Year.Built+Year.Remod.Add+Bedroom.AbvGr , 
                      data=mod_ds,prior = "ZS-null",  #Zellner-Siow Cauchy prior
                      modelprior = uniform())


image(bma_mod_zel_log,rotate=F)

```

* * *
Checking both models, they suggest to remove Land.Slope as a predictor of log(price).
So our final model won't contain Land.Slope and will have BIC as priori because of the greater posteriors odds.

* * *

#

Do you think it is better to log transform Lot.Area, in terms of assumptions for linear regression? Make graphs of the predicted values of log home price versus the true values of log home price for the regression models selected for Lot.Area and log(Lot.Area). Referencing these two plots, provide a written support that includes a quantitative justification for your answer in the first part of question 7.


Let's create the final models with and without log transformation for Lot.Area.

```{r Q7}
bma_mod_zel_log_final <- bas.lm(log(price) ~ log(Lot.Area)+Year.Built+Year.Remod.Add+Bedroom.AbvGr , 
                      data=mod_ds,prior = "BIC",  
                      modelprior = uniform())
bma_mod_zel_final <- bas.lm(log(price) ~ Lot.Area+Year.Built+Year.Remod.Add+Bedroom.AbvGr , 
                      data=mod_ds,prior = "BIC",
                      modelprior = uniform())

```

Now let's plot prediction and residuals for each model.

```{r}
plot(bma_mod_zel_final, which=1, add.smooth=F)
plot(bma_mod_zel_log_final, which=1, add.smooth=F)
```


* * *

So by log transforming Lot.Area we can see we've reduced  residuals variability'


* * *
###